{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reproduce.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B3dMnaMFGtR"
      },
      "outputs": [],
      "source": [
        "!pip install Sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BigBirdTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BigBirdForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import nltk\n",
        "\n",
        "from models import *"
      ],
      "metadata": {
        "id": "ojR46yoyFMF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.chdir('drive/MyDrive/machine_learning')"
      ],
      "metadata": {
        "id": "-tbE0E6ZFOj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JWx3-z1wFYKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # load dataset\n",
        "data_train = pd.read_csv('dataset_binary_train.csv')\n",
        "data_test = pd.read_csv('dataset_binary_test.csv')\n",
        "\n",
        "X_train, y_train = data_train.data.tolist(), data_train.label.tolist()\n",
        "X_test, y_test = data_test.data.tolist(), data_test.label.tolist()"
      ],
      "metadata": {
        "id": "khod0pFOFlfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AkkRuG2vIGpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc(test_dataset, transformer_name, transformer, classifier_name=None, classifier=None, batch_size=64):\n",
        "    \"\"\"This function is used to test each trained model\"\"\"\n",
        "    # create dataloader for tensor dataset\n",
        "    val_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)\n",
        "    \n",
        "    # define device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # use cuda for transformer\n",
        "    transformer = transformer.to(device)\n",
        "\n",
        "    # evaluate model\n",
        "    if classifier is not None:\n",
        "        classifier.eval()\n",
        "    transformer.eval()\n",
        "    \n",
        "    eval_acc = 0\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    criterion = F.cross_entropy\n",
        "    \n",
        "    for b, (x_id, x_mask, y) in enumerate(val_dataloader):\n",
        "        x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if classifier is not None:\n",
        "                word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
        "                logits = classifier(word_embedding)\n",
        "                loss = criterion(logits, y)\n",
        "            else:\n",
        "                outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
        "                loss, logits = outputs['loss'], outputs['logits']\n",
        "        \n",
        "        eval_loss += loss\n",
        "        eval_acc += (logits.max(1)[1] == y).float().mean().item()\n",
        "\n",
        "    print(\"Validation loss: {}\".format(eval_loss / len(val_dataloader)))\n",
        "    print(\"Validation accuracy: {}\".format(eval_acc / len(val_dataloader)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('The test accuracy of {} {} is {}'.format(transformer_name, classifier_name, eval_acc / len(val_dataloader)))"
      ],
      "metadata": {
        "id": "Z8jyAEOBIGsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "efIc-8ipI-HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BERT**"
      ],
      "metadata": {
        "id": "41g6BkViI_mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BERT')\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
        "\n",
        "bert = torch.load('bert-unfreeze.pkl')\n",
        "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
      ],
      "metadata": {
        "id": "jTzrwqNaNRqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned GPT2**"
      ],
      "metadata": {
        "id": "MKOkS9yUOskK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('GPT2')\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
        "\n",
        "gpt2 = torch.load('fc-gpt2.pkl')\n",
        "test_acc(test_dataset, transformer_name='GPT2', transformer=gpt2)"
      ],
      "metadata": {
        "id": "jaTP9r8DI-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BIGBIRD**"
      ],
      "metadata": {
        "id": "RqPLKJTZPCck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BIGBIRD')\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
        "\n",
        "bigbird = torch.load('fc-bigbird-epoch3.pkl')\n",
        "test_acc(test_dataset, transformer_name='BIGBIRD', transformer=bigbird)"
      ],
      "metadata": {
        "id": "m62Cr9lpI-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BERT + BiLSTM**"
      ],
      "metadata": {
        "id": "_X5RrTZHPWlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = torch.load('LSTM-bert-embedding-LSTM.pkl')\n",
        "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = True)\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
        "test_acc(test_dataset, transformer_name='BERT', transformer=bert, classifier_name='BiLSTM', classifier=lstm)\n"
      ],
      "metadata": {
        "id": "zRw3UG1sI-bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BERT + TextCNN**"
      ],
      "metadata": {
        "id": "Sl2AtCTpPvgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textcnn = torch.load('cnn-embedding.pkl')\n",
        "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = True)\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
        "test_acc(test_dataset, transformer_name='BERT', transformer=bert, classifier_name='TextCNN', classifier=textcnn)"
      ],
      "metadata": {
        "id": "0BdIOfkII-di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BERT with large dataset**"
      ],
      "metadata": {
        "id": "gunLQNo_QBpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # load large dataset (~5 million sentences)\n",
        "\n",
        "data_large_train = pd.read_csv('dataset_binary_train_large.csv')\n",
        "data_large_test = pd.read_csv('dataset_binary_test_large.csv')\n",
        "\n",
        "\n",
        "X_large_train, y_large_train = data_large_train.data.tolist(), data_large_train.label.tolist()\n",
        "X_large_test, y_large_test = data_large_test.data.tolist(), data_large_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_large_train)))\n",
        "print('Test dataset length: {}'.format(len(X_large_test)))"
      ],
      "metadata": {
        "id": "pRFar9eMrA4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BERT')\n",
        "bert = torch.load('bert-large.pkl')\n",
        "\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_large_test, y_train=None, y_test=y_large_test)\n",
        "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
      ],
      "metadata": {
        "id": "wJm1Llm1rDMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test fine-tuned BERT with multi-label data**"
      ],
      "metadata": {
        "id": "o-h0Ld2NSpsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # load multi-label dataset\n",
        "\n",
        "data_multi_train = pd.read_csv('dataset_multi_num_train.csv')\n",
        "data_multi_test = pd.read_csv('dataset_multi_num_test.csv')\n",
        "\n",
        "X_multi_train, y_multi_train = data_multi_train.data.tolist(), data_multi_train.label.tolist()\n",
        "X_multi_test, y_multi_test = data_multi_test.data.tolist(), data_multi_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_multi_train)))\n",
        "print('Test dataset length: {}'.format(len(X_multi_test)))"
      ],
      "metadata": {
        "id": "avH4viSUI-jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BERT')\n",
        "bert = torch.load('bert-multi.pkl')\n",
        "\n",
        "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_multi_test, y_train=None, y_test=y_multi_test)\n",
        "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
      ],
      "metadata": {
        "id": "myZVq4iZq-Qb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}