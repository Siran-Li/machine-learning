{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "342ce159-29ab-46d1-87af-7523b44ec8b2",
      "metadata": {
        "id": "342ce159-29ab-46d1-87af-7523b44ec8b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import csv\n",
        "import os\n",
        "import psutil\n",
        "import timeit\n",
        "from datasets import load_dataset\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "from utils import *\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "JGD-F5YYhpcZ"
      },
      "id": "JGD-F5YYhpcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.chdir('drive/MyDrive/machine_learning')"
      ],
      "metadata": {
        "id": "9LTGnzB4jQBC"
      },
      "id": "9LTGnzB4jQBC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "584b8de3-74a9-4915-85c2-75322bf0cbf0",
      "metadata": {
        "id": "584b8de3-74a9-4915-85c2-75322bf0cbf0"
      },
      "source": [
        "### I. Preprocess Raw Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178e7eb7-9e6b-4bce-89c1-6cb8772c1b92",
      "metadata": {
        "id": "178e7eb7-9e6b-4bce-89c1-6cb8772c1b92"
      },
      "source": [
        "##### Pre-process news report data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345824aa-39ff-4432-ad92-694c299c6c67",
      "metadata": {
        "id": "345824aa-39ff-4432-ad92-694c299c6c67"
      },
      "outputs": [],
      "source": [
        "# load news data\n",
        "\n",
        "data_dir = 'datasets/raw-data/news_data'\n",
        "data = []\n",
        "for text_dir in os.listdir(data_dir):\n",
        "    data += pd.read_csv(data_dir + '/' + text_dir, encoding = \"utf-8\", ).content.tolist()\n",
        "\n",
        "data = preprocess(data, length_threshold = 5, select_long = True) # for news data, remove sentences within 5 words\n",
        "\n",
        "# save processed data\n",
        "with open('datasets/processed-data/data_news.txt',\"w\", encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(\"\".join(map(str, x)) for x in data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b87619-e130-4f5d-a3c1-1ac5906f9f82",
      "metadata": {
        "id": "87b87619-e130-4f5d-a3c1-1ac5906f9f82"
      },
      "source": [
        "##### Pre-process dialogue data (Topical-Chat dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e3dc07-6896-4c9b-b595-1b506bea84e2",
      "metadata": {
        "id": "50e3dc07-6896-4c9b-b595-1b506bea84e2"
      },
      "outputs": [],
      "source": [
        "# load second conversation data\n",
        "with open(\"datasets/raw-data/Topical-Chat-master/Topical-Chat-master/conversations/train.json\", encoding='utf-8') as f: \n",
        "    data_temp = json.load(f)\n",
        "\n",
        "# extract only the conversations\n",
        "data = []\n",
        "for dicts in data_temp.keys():\n",
        "    for content in data_temp[dicts]['content']:\n",
        "        data.append(content['message'])\n",
        "        \n",
        "data = preprocess(data, select_long = False, length_threshold = 3) # for dialogue data, remove sentences within 3 words\n",
        "\n",
        "# save processed data\n",
        "with open('datasets/processed-data/data_dialog.txt',\"w\", encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(\"\".join(map(str, x)) for x in data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641624fd-3e8a-4940-b909-30094819d38c",
      "metadata": {
        "id": "641624fd-3e8a-4940-b909-30094819d38c"
      },
      "source": [
        "##### Pre-process Ted Talk data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03845e1a-8e2a-47b4-8d43-374f07e772ae",
      "metadata": {
        "id": "03845e1a-8e2a-47b4-8d43-374f07e772ae"
      },
      "outputs": [],
      "source": [
        "# load English text from ted dataset\n",
        "df = pd.read_csv('datasets/raw-data/ted2020/ted2020.tsv', sep='\\t', keep_default_na=False, encoding='utf8', quoting=csv.QUOTE_NONE)\n",
        "eng_data = df.en.values.flatten().tolist()\n",
        "        \n",
        "data = preprocess(eng_data, select_long = False) # for dialogue data, remove sentences within 3 words\n",
        "\n",
        "# save processed data\n",
        "with open('datasets/processed-data/data_ted.txt',\"w\", encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(\"\".join(map(str, x)) for x in data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9380f0c6-d0aa-4346-a8a4-e418f1344961",
      "metadata": {
        "id": "9380f0c6-d0aa-4346-a8a4-e418f1344961"
      },
      "source": [
        "#####  Pre-process Wikipedia data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5007b733-a588-46c5-983a-bd58c66d6b70",
      "metadata": {
        "id": "5007b733-a588-46c5-983a-bd58c66d6b70"
      },
      "outputs": [],
      "source": [
        "# load wikipedia data\n",
        "wiki = load_dataset(\"wikipedia\", \"20200501.en\", split='train')\n",
        "\n",
        "# select 200,000 random topics\n",
        "index = np.random.randint(len(wiki), size=int(2e5))\n",
        "\n",
        "# preprocess raw text of each selected topic\n",
        "data_final = []\n",
        "\n",
        "for number, i in enumerate(index):\n",
        "    text = wiki[int(i)]['text']\n",
        "    processed_text  = preprocess_text(text)\n",
        "    data_final.extend(processed_text)\n",
        "    print('\\r{}/{} complete'.format(number, len(index)), end='', flush=True)\n",
        "    \n",
        "print('Got {} sentences'.format(len(data_final)))\n",
        "\n",
        "# save processed data\n",
        "with open('datasets/processed-data/data_wikipedia.txt',\"w\", encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(\"\".join(map(str, x)) for x in data_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a786e4b1-7f17-4e9a-b460-a7591e2db406",
      "metadata": {
        "id": "a786e4b1-7f17-4e9a-b460-a7591e2db406"
      },
      "source": [
        "### II. Construct positive and negative samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf78edf4-5c52-4c07-b555-b82e30576dcb",
      "metadata": {
        "id": "cf78edf4-5c52-4c07-b555-b82e30576dcb"
      },
      "source": [
        "##### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78adf7d-f741-4575-92d5-5e482583ab90",
      "metadata": {
        "id": "c78adf7d-f741-4575-92d5-5e482583ab90"
      },
      "outputs": [],
      "source": [
        "# load news data\n",
        "with open(\"datasets/processed-data/data_news.txt\", encoding='utf-8') as f: \n",
        "    data_news = f.readlines() \n",
        "# ignore '\\n'\n",
        "data_news = [sentence[:-1] for sentence in data_news]\n",
        "data_news = list(set(data_news))\n",
        "\n",
        "\n",
        "# load dialog data\n",
        "with open(\"datasets/processed-data/data_dialog.txt\", encoding='utf-8') as f: \n",
        "    data_dialog = f.readlines()\n",
        "# ignore '\\n'\n",
        "data_dialog = [sentence[:-1] for sentence in data_dialog]\n",
        "data_dialog = list(set(data_dialog))\n",
        "\n",
        "\n",
        "# load ted data\n",
        "with open(\"datasets/processed-data/data_ted.txt\", encoding='utf-8') as f: \n",
        "    data_ted = f.readlines()\n",
        "# ignore '\\n'\n",
        "data_ted = [sentence[:-1] for sentence in data_ted]\n",
        "data_ted = list(set(data_ted))\n",
        "\n",
        "\n",
        "# # load wikipedia data\n",
        "with open(\"datasets/processed-data/data_wikidata.txt\", encoding='utf-8') as f: \n",
        "    data_wikipedia = f.readlines()\n",
        "# ignore '\\n'\n",
        "data_wikipedia = [sentence[:-1] for sentence in data_wikipedia]\n",
        "data_wikipedia = list(set(data_wikipedia))\n",
        "\n",
        "# shuffle the data\n",
        "random.shuffle(data_news)\n",
        "random.shuffle(data_dialog)\n",
        "random.shuffle(data_ted)\n",
        "random.shuffle(data_wikipedia)\n",
        "print('Got {} news text data, {} dialog data, {} ted data, {} wikipedia data'.format(len(data_news), len(data_dialog), len(data_ted), len(data_wikipedia)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7ed85c-3e8b-40bc-9458-754a7642c2bc",
      "metadata": {
        "id": "3c7ed85c-3e8b-40bc-9458-754a7642c2bc"
      },
      "source": [
        "##### Create the standard dataset (~ 1 million sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b270fcf7-2365-4c3c-8606-e825e5bede8a",
      "metadata": {
        "id": "b270fcf7-2365-4c3c-8606-e825e5bede8a"
      },
      "outputs": [],
      "source": [
        "# retrieve the sentences from the 4 datasets respectively\n",
        "data_all = data_news[0:500000] + data_ted[0:500000] + data_wikipedia[0:500000] + data_dialog[0:250000]\n",
        "# remove duplicates\n",
        "data_all = list(set(data_all))\n",
        "# shuffle\n",
        "random.shuffle(data_all)\n",
        "# keep only 1.1 million sentences\n",
        "data_all = data_all[0:1100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19bc29e-a4f7-47cb-92e4-decd9902b221",
      "metadata": {
        "id": "d19bc29e-a4f7-47cb-92e4-decd9902b221"
      },
      "outputs": [],
      "source": [
        "# create positive and negative samples\n",
        "\n",
        "# create positive samples\n",
        "pos_samples = []\n",
        "for i, sentence in enumerate(data_all):\n",
        "    pos_samples.append(clean_punctuation(sentence))\n",
        "    print('\\r-------- {}/{} positive samples generated ----------'.format(i+1, len(data_all)), flush=True, end='')\n",
        "    \n",
        "pos_samples = list(set(pos_samples) - set(''))\n",
        "pos_labels = len(pos_samples) * ['complete sentence']\n",
        "\n",
        "# create negative samples\n",
        "neg_samples, neg_labels = generate_negative_samples(data_all.copy())\n",
        "\n",
        "print('Got {} positive samples and {} negative samples'.format(len(pos_samples), len(neg_samples)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2229ea-ad16-4480-aeb0-c7c93c9de667",
      "metadata": {
        "id": "7f2229ea-ad16-4480-aeb0-c7c93c9de667"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative samples\n",
        "samples = pos_samples + neg_samples\n",
        "labels = pos_labels + neg_labels\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "dataset['data'] = samples\n",
        "dataset['label'] = labels\n",
        "\n",
        "# drop na, duplicates, and sentences longer than 100 words\n",
        "dataset.dropna(inplace=True)\n",
        "dataset.drop_duplicates(subset=['data','label'],keep='first',inplace=True)\n",
        "dataset['length'] = [len(sent.split(' ')) for sent in dataset['data']]\n",
        "dataset.drop(dataset[dataset['length'] > 100].index, inplace=True)\n",
        "dataset.drop(axis=1, columns='length', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c07cf61-e105-417d-8a13-679c859a9e59",
      "metadata": {
        "id": "6c07cf61-e105-417d-8a13-679c859a9e59"
      },
      "outputs": [],
      "source": [
        "# save the dataset with multiple labels for negative samples\n",
        "dataset.to_csv('datasets/labeled-data/dataset_multi_label.csv', index=0)\n",
        "\n",
        "# save the dataset with binary labels (complete sentence or not)\n",
        "dataset['label'] = (dataset['label']=='complete sentence') + 0\n",
        "dataset.to_csv('datasets/labeled-data/dataset_binary_label.csv', index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4918c53a-e18a-47ca-83f0-efb333a0cc6b",
      "metadata": {
        "id": "4918c53a-e18a-47ca-83f0-efb333a0cc6b"
      },
      "outputs": [],
      "source": [
        "# create training and test set for binary label\n",
        "\n",
        "dataset = pd.read_csv('datasets/labeled-data/dataset_binary_label.csv')\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "X, y = dataset.data.tolist(), dataset.label.tolist()\n",
        "\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# split 10% of tranining set as ablation set\n",
        "X_train, X_ab, y_train, y_ab = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# save the datasets\n",
        "dataset_train = pd.DataFrame({'data': X_train, 'label':y_train})\n",
        "dataset_ablation = pd.DataFrame({'data': X_ab, 'label':y_ab})\n",
        "dataset_test = pd.DataFrame({'data': X_test, 'label':y_test})\n",
        "\n",
        "dataset_train.to_csv('datasets/labeled-data/dataset_binary_train.csv', index=0)\n",
        "dataset_ablation.to_csv('datasets/labeled-data/dataset_binary_ablation.csv', index=0)\n",
        "dataset_test.to_csv('datasets/labeled-data/dataset_binary_test.csv', index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496fe7c9-89aa-471d-b242-9541fdbc9e59",
      "metadata": {
        "id": "496fe7c9-89aa-471d-b242-9541fdbc9e59"
      },
      "outputs": [],
      "source": [
        "# create training and test set for multi label\n",
        "\n",
        "dataset = pd.read_csv('datasets/labeled-data/dataset_multi_label.csv')\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "X, y = dataset.data.tolist(), dataset.label.tolist()\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "# split 10% of tranining set as ablation set\n",
        "X_train, X_ab, y_train, y_ab = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "dataset_train = pd.DataFrame({'data': X_train, 'label':y_train})\n",
        "dataset_ablation = pd.DataFrame({'data': X_ab, 'label':y_ab})\n",
        "dataset_test = pd.DataFrame({'data': X_test, 'label':y_test})\n",
        "\n",
        "dataset_train.to_csv('datasets/labeled-data/dataset_multi_train.csv', index=0)\n",
        "dataset_ablation.to_csv('datasets/labeled-data/dataset_multi_ablation.csv', index=0)\n",
        "dataset_test.to_csv('datasets/labeled-data/dataset_multi_test.csv', index=0)\n",
        "\n",
        "def convert_to_number(y):\n",
        "    \"\"\" transform the string labels into numbers \"\"\"\n",
        "    mapping = {'complete sentence': 0,\n",
        "               'random cut': 1,\n",
        "               'random missing': 2,\n",
        "               'random repeat': 3,\n",
        "               'random replace': 4}\n",
        "    y_number = [mapping[i] if i in mapping.keys() else i for i in y]\n",
        "    return y_number\n",
        "\n",
        "# transform the string labels into numbers\n",
        "y_train = convert_to_number(y_train)\n",
        "y_ab = convert_to_number(y_ab)\n",
        "y_test = convert_to_number(y_test)\n",
        "\n",
        "# save the datasets\n",
        "dataset_train = pd.DataFrame({'data': X_train, 'label':y_train})\n",
        "dataset_ablation = pd.DataFrame({'data': X_ab, 'label':y_ab})\n",
        "dataset_test = pd.DataFrame({'data': X_test, 'label':y_test})\n",
        "\n",
        "dataset_train.to_csv('datasets/labeled-data/dataset_multi_num_train.csv', index=0)\n",
        "dataset_ablation.to_csv('datasets/labeled-data/dataset_multi_num_ablation.csv', index=0)\n",
        "dataset_test.to_csv('datasets/labeled-data/dataset_multi_num_test.csv', index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c929839a-5fa7-4588-b8a5-61c2fe2a702f",
      "metadata": {
        "id": "c929839a-5fa7-4588-b8a5-61c2fe2a702f"
      },
      "source": [
        "##### Create the large dataset (~ 5 million sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b77af87-d4dc-4fc3-a5b7-73e7130e24fe",
      "metadata": {
        "id": "9b77af87-d4dc-4fc3-a5b7-73e7130e24fe"
      },
      "outputs": [],
      "source": [
        "# retrieve the sentences from the 4 datasets respectively\n",
        "data_all = data_news + data_ted + data_wikipedia[0:2000000] + data_dialog\n",
        "# remove duplicates\n",
        "data_all = list(set(data_all))\n",
        "# shuffle\n",
        "random.shuffle(data_all)\n",
        "len(data_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e6cdff-db11-490d-ae5b-9fa2a4441767",
      "metadata": {
        "id": "d6e6cdff-db11-490d-ae5b-9fa2a4441767"
      },
      "outputs": [],
      "source": [
        "# create positive and negative samples\n",
        "\n",
        "# create positive samples\n",
        "pos_samples = []\n",
        "for i, sentence in enumerate(data_all):\n",
        "    pos_samples.append(clean_punctuation(sentence))\n",
        "    print('\\r-------- {}/{} positive samples generated ----------'.format(i+1, len(data_all)), flush=True, end='')\n",
        "pos_samples = list(set(pos_samples) - set(''))\n",
        "pos_labels = len(pos_samples) * ['complete sentence']\n",
        "\n",
        "# create negative samples\n",
        "neg_samples, neg_labels = generate_negative_samples(data_all.copy())\n",
        "\n",
        "print('Got {} positive samples and {} negative samples'.format(len(pos_samples), len(neg_samples)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4c99bb-5984-4bcd-a514-a552dcd4ea6a",
      "metadata": {
        "id": "9f4c99bb-5984-4bcd-a514-a552dcd4ea6a"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative samples\n",
        "samples = pos_samples + neg_samples\n",
        "labels = pos_labels + neg_labels\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "dataset['data'] = samples\n",
        "dataset['label'] = labels\n",
        "\n",
        "# drop na, duplicates, and sentences longer than 100 words\n",
        "dataset.dropna(inplace=True)\n",
        "dataset.drop_duplicates(subset=['data','label'],keep='first',inplace=True)\n",
        "dataset['length'] = [len(sent.split(' ')) for sent in dataset['data']]\n",
        "dataset.drop(dataset[dataset['length'] > 100].index, inplace=True)\n",
        "dataset.drop(axis=1, columns='length', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a0638e-a011-41e3-9298-60adb776bb7a",
      "metadata": {
        "id": "07a0638e-a011-41e3-9298-60adb776bb7a"
      },
      "outputs": [],
      "source": [
        "# save the dataset with multiple labels for negative samples\n",
        "dataset.to_csv('datasets/labeled-data/dataset_multi_label_large.csv', index=0)\n",
        "\n",
        "# save the dataset with binary labels (complete sentence or not)\n",
        "dataset['label'] = (dataset['label']=='complete sentence') + 0\n",
        "dataset.to_csv('datasets/labeled-data/dataset_binary_label_large.csv', index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf5a491-9f56-4443-b7ae-17d450db1e47",
      "metadata": {
        "id": "daf5a491-9f56-4443-b7ae-17d450db1e47"
      },
      "outputs": [],
      "source": [
        "# create training and test set for binary label\n",
        "\n",
        "dataset = pd.read_csv('datasets/labeled-data/dataset_binary_label_large.csv')\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "X, y = dataset.data.tolist(), dataset.label.tolist()\n",
        "\n",
        "# split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
        "\n",
        "# save the datasets\n",
        "dataset_train = pd.DataFrame({'data': X_train, 'label':y_train})\n",
        "dataset_test = pd.DataFrame({'data': X_test, 'label':y_test})\n",
        "\n",
        "dataset_train.to_csv('datasets/labeled-data/dataset_binary_train_large.csv', index=0)\n",
        "dataset_test.to_csv('datasets/labeled-data/dataset_binary_test_large.csv', index=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Pre-processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}