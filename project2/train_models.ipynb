{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me5HrTfminJ0"
      },
      "outputs": [],
      "source": [
        "!pip install Sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN38mJAQzu6i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BigBirdTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BigBirdForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import nltk\n",
        "\n",
        "from models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2_6Vg30-PC"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrZnQzOl2u9b"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir('drive/MyDrive/machine_learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcrN3SYzizGF"
      },
      "outputs": [],
      "source": [
        "# # load dataset\n",
        "\n",
        "data_train = pd.read_csv('dataset_binary_train.csv')\n",
        "data_test = pd.read_csv('dataset_binary_test.csv')\n",
        "\n",
        "X_train, y_train = data_train.data.tolist(), data_train.label.tolist()\n",
        "X_test, y_test = data_test.data.tolist(), data_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_train)))\n",
        "print('Test dataset length: {}'.format(len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61dZO_rsizPs"
      },
      "outputs": [],
      "source": [
        "def train_val(train_dataset, test_dataset, transformer_name, transformer, classifier_name=None, classifier=None, lr_transformer=3e-5, lr_classifier=1e-3, batch_size=64, max_epoch=5):\n",
        "    # create dataloader for tensor dataset\n",
        "    train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "    val_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)\n",
        "    \n",
        "    # define device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # use cuda for transformer\n",
        "    transformer = transformer.model.to(device)\n",
        "    \n",
        "    # define models\n",
        "    if classifier is not None:\n",
        "        classifier = classifier.to(device)\n",
        "        optimizer = torch.optim.Adam([{\"params\": classifier.parameters(), 'lr': lr_classifier}])\n",
        "        for p in transformer.parameters(): # freeze the layers of transformer\n",
        "            p.requires_grad = False\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(transformer.parameters(), lr = lr_transformer) # the learning rate is suggested by the authors\n",
        "\n",
        "        for p in transformer.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    \n",
        "    # Hyper-parameters\n",
        "    max_epoch = 5\n",
        "    n_batch = int(len(train_dataset)/batch_size)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_dataloader) * max_epoch)\n",
        "    criterion = F.cross_entropy\n",
        "    \n",
        "    # clean memory in GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # a list to record the state of training\n",
        "    training_stats = []\n",
        "\n",
        "\n",
        "            \n",
        "    print('Training start!')\n",
        "    for e in range(max_epoch):\n",
        "        \n",
        "        # train model\n",
        "        #model.train()\n",
        "        if classifier is not None:\n",
        "            classifier.train()\n",
        "        else:\n",
        "            transformer.train()\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        train_acc = 0\n",
        "        \n",
        "        for b, (x_id, x_mask, y) in enumerate(train_dataloader):\n",
        "            x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            if classifier is not None:\n",
        "                with torch.no_grad():\n",
        "                    word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
        "                logits = classifier(word_embedding)\n",
        "                loss = criterion(logits, y)\n",
        "            else:\n",
        "                outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
        "                loss, logits = outputs['loss'], outputs['logits']\n",
        "\n",
        "\n",
        "            epoch_loss += loss\n",
        "            train_acc += (logits.max(1)[1] == y).float().mean().item()\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            #clip gradient\n",
        "            if classifier is None:\n",
        "                torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            print(\"\\rEpoch: {:d} batch: {:d} / {} loss: {:.4f} | {:.2%}\".format(e + 1, b, n_batch, loss, b*1.0/n_batch), end='', flush=True)\n",
        "        print(\"\\n----- Epoch {} ------\\nTraining loss: {}\".format(e+1, epoch_loss / len(train_dataloader)))\n",
        "        print(\"Training accuracy: {}\".format(train_acc / len(train_dataloader)))\n",
        "\n",
        "        \n",
        "        # evaluate model\n",
        "        if classifier is not None:\n",
        "            classifier.eval()\n",
        "        transformer.eval()\n",
        "        \n",
        "        eval_acc = 0\n",
        "        eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        \n",
        "        for b, (x_id, x_mask, y) in enumerate(val_dataloader):\n",
        "            x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                if classifier is not None:\n",
        "                    word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
        "                    logits = classifier(word_embedding)\n",
        "                    loss = criterion(logits, y)\n",
        "                else:\n",
        "                    outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
        "                    loss, logits = outputs['loss'], outputs['logits']\n",
        "            \n",
        "            eval_loss += loss\n",
        "            eval_acc += (logits.max(1)[1] == y).float().mean().item()\n",
        "\n",
        "        print(\"Validation loss: {}\".format(eval_loss / len(val_dataloader)))\n",
        "        print(\"Validation accuracy: {}\".format(eval_acc / len(val_dataloader)))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': e+1,\n",
        "                'train_loss': epoch_loss / len(train_dataloader),\n",
        "                'train_acc': train_acc / len(train_dataloader),\n",
        "                'val_loss': eval_loss / len(val_dataloader),\n",
        "                'val_acc': eval_acc / len(val_dataloader),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # save models\n",
        "        if classifier is not None:\n",
        "            torch.save(classifier, '{}-{}.pkl'.format(transformer_name, classifier_name))\n",
        "        else:\n",
        "            torch.save(transformer, '{}.pkl'.format(transformer_name))\n",
        "        \n",
        "        # save states of training\n",
        "        np.save('{}-{}-train_stats_Epoch{}.npy'.format(transformer_name, classifier_name, e+1), training_stats) \n",
        "\n",
        "    print('Training complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur4YhIiz1B0i"
      },
      "source": [
        "**Fine-tune BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PApSKotb1For"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqhDJZs1GD4"
      },
      "source": [
        "**Fine-tune GPT2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp3pA1_G1NlM"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer('GPT2')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='GPT2', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oalx5Ww41N7S"
      },
      "source": [
        "**Fine-tune BIGBIRD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjKJpHod1QmD"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer('BIGBIRD')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BIGBIRD', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQKmQuT1Q7t"
      },
      "source": [
        "**BERT + BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-gMHbyR1YSe"
      },
      "outputs": [],
      "source": [
        "lstm = LSTM_attention()\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer, classifier_name='BiLSTM', classifier=lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4PfEBBs1Yex"
      },
      "source": [
        "**BERT + TextCNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKmzAhzh2hoe"
      },
      "outputs": [],
      "source": [
        "textcnn = textCNN()\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer, classifier_name='TextCNN', classifier=textcnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LBnlb3LVAoo"
      },
      "source": [
        "**Fine-tune BERT with large dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T66JemkOU72B"
      },
      "outputs": [],
      "source": [
        "# # load large dataset (~5 million sentences)\n",
        "\n",
        "data_large_train = pd.read_csv('dataset_binary_train_large.csv')\n",
        "data_large_test = pd.read_csv('dataset_binary_test_large.csv')\n",
        "\n",
        "\n",
        "X_large_train, y_large_train = data_large_train.data.tolist(), data_large_train.label.tolist()\n",
        "X_large_test, y_large_test = data_large_test.data.tolist(), data_large_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_large_train)))\n",
        "print('Test dataset length: {}'.format(len(X_large_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0-toOtTVGGW"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_large_train, X_large_test, y_large_train, y_large_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB7X7k9gVIgm"
      },
      "source": [
        "**Fine-tune BERT with multi-label data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHiOSW4GtgMQ"
      },
      "outputs": [],
      "source": [
        "# # load multi-label dataset\n",
        "\n",
        "data_multi_train = pd.read_csv('dataset_multi_num_train.csv')\n",
        "data_multi_test = pd.read_csv('dataset_multi_num_test.csv')\n",
        "\n",
        "X_multi_train, y_multi_train = data_multi_train.data.tolist(), data_multi_train.label.tolist()\n",
        "X_multi_test, y_multi_test = data_multi_test.data.tolist(), data_multi_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_multi_train)))\n",
        "print('Test dataset length: {}'.format(len(X_multi_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym_KJB-kVPfl"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer('BERT', num_labels=5)\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_multi_train, X_multi_test, y_multi_train, y_multi_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}