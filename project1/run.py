import numpy as np
from proj1_helpers import load_csv_data, create_csv_submission
from implementations import *


def fea_augmentation(x, method, poly_degree=None, neg_poly_degree=None, concatenate=True, print_info=False,
                     standarize=True):
    x_aug = x.copy() if concatenate else []

    if 'polynomial' in method:
        for d in range(2, poly_degree + 1):
            try:
                x_aug = np.c_[x_aug, x ** d]
            except:
                x_aug = x ** d

    if 'cross_product_degree_2' in method:
        for i in range(x.shape[1]):
            for j in range(i + 1, x.shape[1]):
                x_aug = np.c_[x_aug, x[:, i] * x[:, j]]
            if print_info:
                print(
                    '\rAdding degree-two cross product features | {:.2%} complete'.format(float((i + 1) / x.shape[1])),
                    end='', flush=True)
        print('')

    if 'cross_product_degree_3' in method:
        for i in range(x.shape[1]):
            if i != 1:
                x_aug = np.c_[x_aug, x[:, 1] * (x[:, i] ** 2)]
                x_aug = np.c_[x_aug, (x[:, 1] ** 2) * x[:, i]]
                if (i != 11):
                    x_aug = np.c_[x_aug, x[:, 11] * (x[:, i] ** 2)]
                    x_aug = np.c_[x_aug, (x[:, 11] ** 2) * x[:, i]]
                    if (i != 13):
                        x_aug = np.c_[x_aug, x[:, 13] * (x[:, i] ** 2)]
                        x_aug = np.c_[x_aug, (x[:, 13] ** 2) * x[:, i]]
                        if (i != 22):
                            x_aug = np.c_[x_aug, x[:, 22] * (x[:, i] ** 2)]
                            x_aug = np.c_[x_aug, (x[:, 22] ** 2) * x[:, i]]
            if print_info:
                print('\rAdding degree-three cross product features | {:.2%} complete'.format(
                    float((i + 1) / x.shape[1])), end='', flush=True)
        print('')

    if 'missing_value_indicator' in method:
        x_aug = np.c_[x_aug, indicator_feature_1]
        x_aug = np.c_[x_aug, indicator_feature_2]

    if 'neg_polynomial' in method:
        for d in range(1, neg_poly_degree + 1):
            try:
                x_aug = np.c_[x_aug, 1 / (x ** d + 1e-4)]
            except:
                x_aug = 1 / (x ** d + 1e-4)

    x_aug_mean = x_aug.mean(axis=0)
    x_aug_std = x_aug.std(axis=0)

    # re-standarize to avoid numerical problems for gradient descent
    if standarize == True:
        x_aug = (x_aug - x_aug.mean(axis=0)) / x_aug.std(axis=0)

    # add a constant column to the features
    if concatenate:
        x_aug = np.c_[np.ones((len(x_aug), 1)), x_aug]

    return x_aug, x_aug_mean, x_aug_std

def expand_over_jetnum(x, jet_n):
    """ This function expands the features over the jet_num feature. The details of this expansion is discussed in the markdown cells. """
    x_expand = np.zeros((x.shape[0], 4*x.shape[1]))
    for n, j_n in enumerate(jet_n):
        x_expand[n, int(j_n*x.shape[1]):int((j_n+1)*x.shape[1])] = x[n, :]
    return x_expand

def main():
    # load data
    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)

    # Feature engineering
    # to avoid running the same process many times: 
    # first, we generate x_aug_temp using feauture augmentation methods except polynomial expansion and negative powers, 
    # when doing cross validation x_aug_temp is concatenated to feautures generated by polynomial expansion and negative powers

    # indicator_feature_1 = np.int32(tX[:, 0] == -999)
    # indicator_feature_2 = np.int32(tX[:, 23] == -999) + np.int32(tX[:, 24] == -999) + np.int32(tX[:, 25] == -999)
    jet_num_train = tX[:, 22]


    print('Doing feature augmentation for '
          'training set\n')

    # standarize training set
    x_std, train_means, train_std, train_median = clean_standarize(tX, replace='median')
    x_aug, x_aug_train_mean, x_aug_train_std = fea_augmentation(x_std, method=['polynomial', 'neg_polynomial',
                                                                               'cross_product_degree_2',
                                                                               'cross_product_degree_3'],
                                                                poly_degree=10, neg_poly_degree=8, print_info=True,
                                                                concatenate=True)

    x_aug = expand_over_jetnum(x_aug, jet_num_train)

    # Set hyper-parameters
    lambd = 0.0001

    print("Start training with Ridge Regression: ")
    # Ridge Regression
    w_ridge, _ = ridge_regression(y, x_aug, lambd)
    print('Training complete!')

    # Feature engineering of test set

    DATA_TEST_PATH = 'data/test.csv'
    _, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)

    # indicator_feature_1 = np.int32(tX_test[:, 0] == -999)
    # indicator_feature_2 = np.int32(tX_test[:, 23] == -999) + np.int32(tX_test[:, 24] == -999) + np.int32(
    #     tX_test[:, 25] == -999)
    jet_num_test = tX_test[:, 22]

    print('Doing feature augmentation for test set\n')
    # standarize test set
    x_test_std, _, _, _ = clean_standarize(tX_test, replace='median', mode='test_set', fea_means=train_means,
                                           fea_std=train_std, fea_replace=train_median)
    x_test_aug, _, _ = fea_augmentation(x_test_std, method=['polynomial', 'neg_polynomial',
                                                            'cross_product_degree_2', 'cross_product_degree_3'],
                                        poly_degree=10, neg_poly_degree=8, print_info=True, concatenate=True,
                                        standarize=False)

    # re-standarize test set
    x_test_aug = x_test_aug[:, 1:]
    x_test_aug = (x_test_aug - x_aug_train_mean) / x_aug_train_std
    x_test_aug = np.c_[np.ones((len(x_test_aug), 1)), x_test_aug]

    x_test_aug = expand_over_jetnum(x_test_aug, jet_num_test)

    # Create the submission file
    OUTPUT_PATH = 'y_pred.csv'
    y_pred = predict_labels(x_test_aug, w_ridge, method='linear')
    create_csv_submission(ids_test, y_pred, OUTPUT_PATH)

    print('The predictions of test set are completed!')


if __name__ == "__main__":
    main()